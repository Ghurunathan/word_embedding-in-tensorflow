{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First you have to build your vocab\n",
    "#for example I am going to take only five sentence as dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = ['I like going to the cinema.',\n",
    "           'Mum is going to cook spinach tonight.',\n",
    "           'Do you think they are going to win the match ?',\n",
    "           'Are you going to the United States ?',\n",
    "           \"He doesn't like playing video games.\"\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "#build vocab \n",
    "\n",
    "#you can also remove stopwords if you want\n",
    "\n",
    "vocab=[]\n",
    "\n",
    "symbols = {0: 'PAD',1: 'UNK'}\n",
    "\n",
    "for sentence in dataset:\n",
    "    for word in sentence.split():\n",
    "        vocab.append(word.lower())\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "int_to_vocab = {}\n",
    "\n",
    "for index_no,word in enumerate(vocab,2):\n",
    "    int_to_vocab[index_no] = word\n",
    "int_to_vocab.update(symbols)\n",
    "\n",
    "\n",
    "vocab_to_int = {word:index_no for index_no , word in int_to_vocab.items()}\n",
    "\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "#first method\n",
    "\n",
    "import numpy as np\n",
    "#load glove or any word embedding\n",
    "\n",
    "def word_embedding_matrix(embedding_path,vocab,dim):\n",
    "    \n",
    "    #first and second vector are pad and unk words\n",
    "    \n",
    "    with open(embedding_path,'r') as f:\n",
    "        word_vocab =[]\n",
    "        embedding_matrix = []\n",
    "        word_vocab.extend(['PAD','UNK'])\n",
    "        embedding_matrix.append(np.random.uniform(-1.0, 1.0, (1,100))[0])\n",
    "        embedding_matrix.append(np.random.uniform(-1.0, 1.0, (1,100))[0])\n",
    "\n",
    "        \n",
    "        for line in f:\n",
    "            if line.split()[0] in vocab:\n",
    "                word_vocab.append(line.split()[0])\n",
    "                embedding_matrix.append([float(i) for i in line.split()[1:]])\n",
    "        \n",
    "    return {'word_vocab': word_vocab,'word_vocabulary': np.reshape(embedding_matrix,[-1,100]).astype(np.float32)}\n",
    "\n",
    "print(word_embedding_matrix('./glove.6B.100d.txt',vocab,100)['word_vocabulary'][2])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "#second method\n",
    "\n",
    "def word_embedding_matrix(path,dim):\n",
    "    word_id = {}\n",
    "    with open(path) as fp:\n",
    "        vocabulary = []\n",
    "        embedding_matrix = [np.random.uniform(-1.0, 1.0, (dim, 1))]\n",
    "        for line in fp.readlines():\n",
    "            if line.split()[0] in vocab:\n",
    "                vocabulary.append(line.split()[0])\n",
    "                \n",
    "                embedding_matrix.append(np.reshape(np.array(line.split()[1:]), (dim, 1)))\n",
    "    word_id['PAD'] = 0\n",
    "    word_id['UNK'] = len(vocabulary) + 1\n",
    "    embedding_matrix.append(np.random.uniform(-1.0, 1.0, (dim, 1)))\n",
    "    embedding_matrix = np.reshape(embedding_matrix, [-1, dim])\n",
    "    embedding_matrix = embedding_matrix.astype(np.float32)\n",
    "\n",
    "        \n",
    "    return embedding_matrix , vocabulary\n",
    "\n",
    "embedding_matrix,words2ids = word_embedding_matrix('./glove.6B.100d.txt',100)\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
